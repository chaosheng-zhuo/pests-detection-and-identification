{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4a4a48",
   "metadata": {},
   "source": [
    "### AgroPest-12 -- Detection + Classification pipeline (traditional ML)\n",
    "\n",
    "This object-oriented Python module implements:\n",
    "- Feature extractors: HOG, SIFT (Bag-of-visual-words), LBP\n",
    "- Classifier wrappers: SVM, RandomForest, KNN\n",
    "- Detector: sliding-window multi-scale detector + NMS\n",
    "- Training pipeline: crop annotated boxes to build training samples\n",
    "- Test evaluation: compute per-class AP and mAP based on IoU\n",
    "- Model persistence: saves 9 classifier models + SIFT codebook\n",
    "\n",
    "#### Requirements\n",
    "------------\n",
    "python >=3.8\n",
    "pip install opencv-contrib-python scikit-image scikit-learn joblib tqdm numpy\n",
    "\n",
    "#### Notes\n",
    "-----\n",
    "- SIFT requires opencv-contrib (cv2.SIFT_create).\n",
    "- This module assumes dataset is organized as:\n",
    "  dataset/\n",
    "------ \n",
    "    train/\n",
    "      images/\n",
    "      labels/   # YOLO-like txt or custom CSV. See README below\n",
    "    valid/\n",
    "      images/\n",
    "      labels/\n",
    "    test/\n",
    "      images/\n",
    "      labels/\n",
    "\n",
    "Label format used by the pipeline: CSV per dataset split with fields:\n",
    "image_path,x_min,y_min,x_max,y_max,class_id\n",
    "(one row per bounding box). The pipeline will also create its own crops.\n",
    "\n",
    "This is a working, self-contained implementation. For large datasets, SIFT codebook kmeans\n",
    "training can be slow; you may want to subsample descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe3f0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# AgroPest-12 — Detection & Classification Framework\n",
    "# 作者: weichen wang\n",
    "# 日期: 2025\n",
    "# 支持 YOLO 格式标签 | 自动选最佳模型 | 完整指标报告\n",
    "# ================================================\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import dump, load\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import yaml\n",
    "import math\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15936dd",
   "metadata": {},
   "source": [
    "####  Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81e74ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA + 1)\n",
    "    interH = max(0, yB - yA + 1)\n",
    "    interArea = interW * interH\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    denom = boxAArea + boxBArea - interArea\n",
    "    return interArea / denom if denom > 0 else 0.0\n",
    "\n",
    "def non_max_suppression(boxes, scores, iou_thresh=0.4):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    idxs = np.argsort(scores)[::-1]\n",
    "    keep = []\n",
    "    while len(idxs) > 0:\n",
    "        i = idxs[0]\n",
    "        keep.append(i)\n",
    "        if len(idxs) == 1:\n",
    "            break\n",
    "        rest = idxs[1:]\n",
    "        iou_scores = [iou(boxes[i], boxes[j]) for j in rest]\n",
    "        idxs = np.array(rest)[np.array(iou_scores) <= iou_thresh]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743446f",
   "metadata": {},
   "source": [
    "#### Feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29d3de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, size=(128, 128), mode='hog'):\n",
    "        self.size = size\n",
    "        self.mode = mode  # 'hog' or 'lbp'\n",
    "\n",
    "    def extract_hog(self, img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, self.size)\n",
    "        feat = hog(img, pixels_per_cell=(16, 16), cells_per_block=(2, 2), feature_vector=True)\n",
    "        return feat\n",
    "\n",
    "    def extract_lbp(self, img, P=8, R=1):\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, self.size)\n",
    "        lbp = local_binary_pattern(img, P, R, method='uniform')\n",
    "        n_bins = int(lbp.max() + 1)\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "        return hist\n",
    "\n",
    "class SIFTBoWExtractor:\n",
    "    def __init__(self, size=(128, 128), k=64):\n",
    "        self.size = size\n",
    "        self.k = k\n",
    "        self.codebook = None\n",
    "        self.sift = cv2.SIFT_create()\n",
    "\n",
    "    def compute_descriptors(self, img):\n",
    "        if len(img.shape) == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = img\n",
    "        gray = cv2.resize(gray, self.size)\n",
    "        kp, des = self.sift.detectAndCompute(gray, None)\n",
    "        return des\n",
    "\n",
    "    def fit_codebook(self, descriptor_list, k=None, batch_size=2048, max_iter=300):\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "        all_desc = np.vstack(descriptor_list)\n",
    "        n_samples = min(100000, len(all_desc))  # 上限10万，防止OOM\n",
    "        idxs = np.random.choice(len(all_desc), n_samples, replace=False)\n",
    "        subs = all_desc[idxs]\n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, max_iter=max_iter, random_state=42)\n",
    "        kmeans.fit(subs)\n",
    "        self.codebook = kmeans\n",
    "        return kmeans\n",
    "\n",
    "    def extract_bow(self, img):\n",
    "        des = self.compute_descriptors(img)\n",
    "        if des is None or self.codebook is None or len(des) == 0:\n",
    "            return np.zeros(self.k)\n",
    "        labels = self.codebook.predict(des)\n",
    "        hist, _ = np.histogram(labels, bins=np.arange(self.k + 1), density=True)\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf44a7",
   "metadata": {},
   "source": [
    "####  Classifier wrappers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd99290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierWrapper:\n",
    "    def __init__(self, model_type='svm'):\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.need_pca = False  # 只有 HOG 需要 PCA\n",
    "\n",
    "    def build(self, X_sample=None):  # 传入几行样本用来计算 PCA\n",
    "        if self.model_type == 'svm':\n",
    "            base_svc = LinearSVC(C=1.0, class_weight='balanced', max_iter=10000, dual=False)\n",
    "            self.model = CalibratedClassifierCV(base_svc, method='sigmoid', cv=3, n_jobs=-1)\n",
    "\n",
    "        elif self.model_type == 'rf':\n",
    "            self.model = RandomForestClassifier(\n",
    "                n_estimators=300, n_jobs=-1, class_weight='balanced',\n",
    "                max_depth=20, min_samples_leaf=2, random_state=42\n",
    "            )\n",
    "\n",
    "        elif self.model_type == 'knn':\n",
    "            self.model = KNeighborsClassifier(n_neighbors=7, n_jobs=-1, weights='distance')\n",
    "\n",
    "        # 只有 HOG 才加 PCA\n",
    "        if X_sample is not None and self.model_type != 'knn':  # KNN 不怕高维\n",
    "            if X_sample.shape[1] > 300:  # 只有 HOG > 3000 维\n",
    "                print(f\"Applying PCA for high-dim feature ({X_sample.shape[1]} → ~150 dims)\")\n",
    "                pca = PCA(n_components=0.99, svd_solver='full', whiten=True, random_state=42)\n",
    "                self.model = Pipeline([('pca', pca), ('clf', self.model)])\n",
    "                self.need_pca = True\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.model is None:\n",
    "            self.build(X_sample=X)\n",
    "        print(f\"Training {self.model_type.upper()} on {X.shape[0]} samples, {X.shape[1]} dims...\", end='')\n",
    "        self.model.fit(X, y)\n",
    "        print(\"Done\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def save(self, path):\n",
    "        dump(self.model, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee81ec",
   "metadata": {},
   "source": [
    "####  Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c170621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDetector:\n",
    "    def __init__(self, fe, clf, window_size=(128, 128), step=24, scales=[0.5, 0.75, 1.0, 1.25, 1.5]):\n",
    "        self.fe = fe\n",
    "        self.clf = clf\n",
    "        self.window_size = window_size\n",
    "        self.step = step\n",
    "        self.scales = scales\n",
    "\n",
    "    def _extract(self, crop):\n",
    "        if isinstance(self.fe, SIFTBoWExtractor):\n",
    "            return self.fe.extract_bow(crop)\n",
    "        elif self.fe.mode == 'hog':\n",
    "            return self.fe.extract_hog(crop)\n",
    "        elif self.fe.mode == 'lbp':\n",
    "            return self.fe.extract_lbp(crop)\n",
    "\n",
    "    def detect(self, image, score_thresh=0.65, nms_iou=0.4):\n",
    "        boxes, scores, labels = [], [], []\n",
    "        H, W = image.shape[:2]\n",
    "\n",
    "        for scale in self.scales:\n",
    "            resized = cv2.resize(image, (int(W * scale), int(H * scale)))\n",
    "            h, w = resized.shape[:2]\n",
    "            ws, hs = self.window_size\n",
    "\n",
    "            for y in range(0, h - hs + 1, self.step):\n",
    "                for x in range(0, w - ws + 1, self.step):\n",
    "                    crop = resized[y:y+hs, x:x+ws]\n",
    "                    feat = self._extract(crop)\n",
    "                    if feat is None:\n",
    "                        continue\n",
    "                    prob = self.clf.predict_proba(feat.reshape(1, -1))[0]\n",
    "                    score = prob.max()\n",
    "                    if score >= score_thresh:\n",
    "                        # 逆投影到原图坐标\n",
    "                        orig_x = int(x / scale)\n",
    "                        orig_y = int(y / scale)\n",
    "                        orig_w = int(ws / scale)\n",
    "                        orig_h = int(hs / scale)\n",
    "                        boxes.append([orig_x, orig_y, orig_x + orig_w - 1, orig_y + orig_h - 1])\n",
    "                        scores.append(score)\n",
    "                        labels.append(int(prob.argmax()))\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        boxes = np.array(boxes)\n",
    "        scores = np.array(scores)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        final_boxes, final_scores, final_labels = [], [], []\n",
    "        for cls in np.unique(labels):\n",
    "            idxs = np.where(labels == cls)[0]\n",
    "            keep = non_max_suppression(boxes[idxs], scores[idxs], nms_iou)\n",
    "            for k in keep:\n",
    "                final_boxes.append(boxes[idxs[k]])\n",
    "                final_scores.append(scores[idxs[k]])\n",
    "                final_labels.append(cls)\n",
    "\n",
    "        return final_boxes, final_scores, final_labels\n",
    "\n",
    "\n",
    "class SelectiveSearchDetector:\n",
    "    def __init__(self, fe, clf, max_proposals=1200, score_thresh=0.05, nms_iou=0.4):\n",
    "        self.fe = fe\n",
    "        self.clf = clf\n",
    "        self.max_proposals = max_proposals      # 关键1：限制数量\n",
    "        self.score_thresh = score_thresh         # 关键2：降低阈值\n",
    "        self.nms_iou = nms_iou\n",
    "        self.fixed_size = (224, 224)             # 关键3：固定输入大小\n",
    "\n",
    "    def _extract_feat(self, crop):\n",
    "        # 强制 resize 到 224×224（SIFT/HOG 都够用，还超快！）\n",
    "        if crop.shape[0] == 0 or crop.shape[1] == 0:\n",
    "            return None\n",
    "        crop = cv2.resize(crop, self.fixed_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if isinstance(self.fe, SIFTBoWExtractor):\n",
    "            return self.fe.extract_bow(crop)\n",
    "        elif self.fe.mode == 'hog':\n",
    "            return self.fe.extract_hog(crop)\n",
    "        else:\n",
    "            return self.fe.extract_lbp(crop)\n",
    "\n",
    "    def detect(self, image):\n",
    "        # 1. 极速 Selective Search\n",
    "        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "        ss.setBaseImage(image)\n",
    "        ss.switchToSelectiveSearchFast()          # 必须是 Fast\n",
    "        rects = ss.process()[:self.max_proposals] # 关键：只取前 1200 个！！！\n",
    "\n",
    "        # 2. 批量特征提取 + 预测（向量化）\n",
    "        features = []\n",
    "        boxes = []\n",
    "        for (x, y, w, h) in rects:\n",
    "            if min(w, h) < 20:   # 太小直接扔\n",
    "                continue\n",
    "            crop = image[y:y+h, x:x+w]\n",
    "            feat = self._extract_feat(crop)\n",
    "            if feat is not None:\n",
    "                features.append(feat)\n",
    "                boxes.append([x, y, x+w, y+h])\n",
    "\n",
    "        if len(features) == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        features = np.array(features)\n",
    "        probs = self.clf.predict_proba(features)\n",
    "        scores = probs.max(axis=1)\n",
    "        labels = probs.argmax(axis=1)\n",
    "\n",
    "        # 3. 阈值过滤 + NMS\n",
    "        keep = scores >= self.score_thresh\n",
    "        if not keep.any():\n",
    "            return [], [], []\n",
    "\n",
    "        final_boxes = [boxes[i] for i in range(len(boxes)) if keep[i]]\n",
    "        final_scores = scores[keep].tolist()\n",
    "        final_labels = labels[keep].tolist()\n",
    "\n",
    "        # NMS（用 OpenCV 的超快版本）\n",
    "        if len(final_boxes) > 0:\n",
    "            keep_idx = cv2.dnn.NMSBoxes(\n",
    "                final_boxes, final_scores, score_threshold=0.0, nms_threshold=self.nms_iou\n",
    "            )\n",
    "            if len(keep_idx) > 0:\n",
    "                keep_idx = keep_idx.flatten()\n",
    "                final_boxes = [final_boxes[i] for i in keep_idx]\n",
    "                final_scores = [final_scores[i] for i in keep_idx]\n",
    "                final_labels = [final_labels[i] for i in keep_idx]\n",
    "\n",
    "        return final_boxes, final_scores, final_labels\n",
    "\n",
    "class MSERDetector:\n",
    "    def __init__(self, fe, clf, min_area=500, max_area=50000, score_thresh=0.65, nms_iou=0.4):\n",
    "        self.fe = fe\n",
    "        self.clf = clf\n",
    "        self.min_area = min_area\n",
    "        self.max_area = max_area\n",
    "        self.score_thresh = score_thresh\n",
    "        self.nms_iou = nms_iou\n",
    "        self.mser = cv2.MSER_create()\n",
    "\n",
    "    def _extract(self, crop):\n",
    "        if isinstance(self.fe, SIFTBoWExtractor):\n",
    "            return self.fe.extract_bow(crop)\n",
    "        elif self.fe.mode == 'hog':\n",
    "            return self.fe.extract_hog(crop)\n",
    "        else:\n",
    "            return self.fe.extract_lbp(crop)\n",
    "\n",
    "    def detect(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        regions, _ = self.mser.detectRegions(gray)\n",
    "\n",
    "        boxes, scores, labels = [], [], []\n",
    "\n",
    "        for contour in regions:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            if not (self.min_area <= w * h <= self.max_area):\n",
    "                continue\n",
    "            crop = image[y:y+h, x:x+w]\n",
    "            feat = self._extract(crop)\n",
    "            if feat is None:\n",
    "                continue\n",
    "            prob = self.clf.predict_proba(feat.reshape(1, -1))[0]\n",
    "            score = prob.max()\n",
    "            if score >= self.score_thresh:\n",
    "                boxes.append([x, y, x+w-1, y+h-1])\n",
    "                scores.append(score)\n",
    "                labels.append(int(prob.argmax()))\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        boxes = np.array(boxes)\n",
    "        scores = np.array(scores)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        final_boxes, final_scores, final_labels = [], [], []\n",
    "        for cls in np.unique(labels):\n",
    "            idxs = np.where(labels == cls)[0]\n",
    "            keep = non_max_suppression(boxes[idxs], scores[idxs], self.nms_iou)\n",
    "            for k in keep:\n",
    "                final_boxes.append(boxes[idxs[k]])\n",
    "                final_scores.append(scores[idxs[k]])\n",
    "                final_labels.append(cls)\n",
    "\n",
    "        return final_boxes, final_scores, final_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc11441",
   "metadata": {},
   "source": [
    "#### Yolo Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "803391dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset:\n",
    "    \"\"\"\n",
    "    直接读取你现在的目录结构：\n",
    "    AgroPest-12/\n",
    "      ├── train/\n",
    "      │   ├── images/\n",
    "      │   └── labels/   (.txt)\n",
    "      ├── valid/\n",
    "      └── test/\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir='AgroPest-12', split='train'):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.img_dir = os.path.join(root_dir, split, 'images')\n",
    "        self.lbl_dir = os.path.join(root_dir, split, 'labels')\n",
    "        \n",
    "        # 读取类别名和数量\n",
    "        yaml_path = os.path.join(root_dir, 'data.yaml')\n",
    "        with open(yaml_path) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "            self.names = data['names']\n",
    "            self.num_classes = len(self.names)\n",
    "        \n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        \"\"\"返回列表：(image_path, x1, y1, x2, y2, class_id)\"\"\"\n",
    "        samples = []\n",
    "        lbl_paths = sorted(glob.glob(os.path.join(self.lbl_dir, '*.txt')))\n",
    "        \n",
    "        for lbl_path in lbl_paths:\n",
    "            img_name = os.path.splitext(os.path.basename(lbl_path))[0] + '.jpg'  # 支持 .png 改成下面\n",
    "            # img_name = os.path.splitext(os.path.basename(lbl_path))[0] + '.png'\n",
    "            img_path = os.path.join(self.img_dir, img_name)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "                \n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            with open(lbl_path) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        continue\n",
    "                    cls_id = int(parts[0])\n",
    "                    x_center_norm, y_center_norm, bw_norm, bh_norm = map(float, parts[1:])\n",
    "                    \n",
    "                    x1 = int((x_center_norm - bw_norm / 2) * w)\n",
    "                    y1 = int((y_center_norm - bh_norm / 2) * h)\n",
    "                    x2 = int((x_center_norm + bw_norm / 2) * w)\n",
    "                    y2 = int((y_center_norm + bh_norm / 2) * h)\n",
    "                    \n",
    "                    # 防止越界\n",
    "                    x1 = max(0, x1)\n",
    "                    y1 = max(0, y1)\n",
    "                    x2 = min(w - 1, x2)\n",
    "                    y2 = min(h - 1, y2)\n",
    "                    \n",
    "                    samples.append((img_path, x1, y1, x2, y2, cls_id))\n",
    "        return samples\n",
    "\n",
    "    def get_image_list(self):\n",
    "        return sorted(list(set([s[0] for s in self.samples])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569f356",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50097dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLDetectionPipeline:\n",
    "    def __init__(self, num_classes=12, feature='hog', classifier='svm', sift_k=256):\n",
    "        self.num_classes = num_classes\n",
    "        self.feature = feature\n",
    "        self.classifier_name = classifier\n",
    "        self.sift_k = sift_k\n",
    "\n",
    "        if feature in ['hog', 'lbp']:\n",
    "            self.fe = FeatureExtractor(mode=feature)\n",
    "        elif feature == 'sift':\n",
    "            self.fe = SIFTBoWExtractor(k=sift_k)\n",
    "        else:\n",
    "            raise ValueError('Unknown feature')\n",
    "\n",
    "        self.clf = ClassifierWrapper(classifier)\n",
    "        self.clf.build()\n",
    "\n",
    "    def build_training_data_from_annotations(self, data_root='AgroPest-12', split='train'):\n",
    "        \"\"\"\n",
    "        直接从 YOLO 格式读取训练样本\n",
    "        \"\"\"\n",
    "        ds = YOLODataset(root_dir=data_root, split=split)\n",
    "\n",
    "        # SIFT codebook 共享（只建一次）\n",
    "        if self.feature == 'sift' and self.fe.codebook is None:\n",
    "            codebook_path = os.path.join('models', 'sift_codebook.joblib')\n",
    "            if os.path.exists(codebook_path):\n",
    "                self.fe.codebook = load(codebook_path)\n",
    "                print('Loaded shared SIFT codebook')\n",
    "            else:\n",
    "                print('Building shared SIFT codebook from training set...')\n",
    "                descriptor_pool = []\n",
    "                for img_path, x1, y1, x2, y2, _ in tqdm(ds.samples, desc='Collect SIFT desc'):\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None: continue\n",
    "                    crop = img[y1:y2, x1:x2]\n",
    "                    des = self.fe.compute_descriptors(crop)\n",
    "                    if des is not None and len(des) > 0:\n",
    "                        descriptor_pool.append(des)\n",
    "                if descriptor_pool:\n",
    "                    self.fe.fit_codebook(descriptor_pool)\n",
    "                    dump(self.fe.codebook, codebook_path)\n",
    "                    print('Shared codebook saved')\n",
    "                else:\n",
    "                    print('Warning: No SIFT descriptors collected')\n",
    "\n",
    "        # 正式提取特征\n",
    "        X, y = [], []\n",
    "        for img_path, x1, y1, x2, y2, cls in tqdm(ds.samples, desc='Extracting features'):\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            if crop.size == 0: continue\n",
    "\n",
    "            if self.feature == 'hog':\n",
    "                feat = self.fe.extract_hog(crop)\n",
    "            elif self.feature == 'lbp':\n",
    "                feat = self.fe.extract_lbp(crop)\n",
    "            elif self.feature == 'sift':\n",
    "                feat = self.fe.extract_bow(crop)\n",
    "            else:\n",
    "                continue\n",
    "            X.append(feat)\n",
    "            y.append(cls)\n",
    "\n",
    "        print(f'Extracted {len(X)} samples for {self.feature}+{self.classifier_name}')\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # ==================== train() 支持验证集早停/选模型 ====================\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        新增：支持验证集，早停或选最佳模型\n",
    "        \"\"\"\n",
    "        self.clf.build(X_sample=X_train)\n",
    "\n",
    "        if X_val is None or y_val is None:\n",
    "            # 没有验证集 → 直接训练\n",
    "            print(\"No validation set, training on full data...\")\n",
    "            self.clf.fit(X_train, y_train)\n",
    "            best_score = 0\n",
    "        else:\n",
    "            # 有验证集 → 训练多个候选，取验证集最好的\n",
    "            print(\"Training with validation set selection...\")\n",
    "            best_score = 0\n",
    "            best_model = None\n",
    "\n",
    "            # 这里我们用一个简单的训练策略：训练一个就保存最好的\n",
    "            # 对于 RF/SVM 我们只训一次，但可以用 val 评估\n",
    "            self.clf.fit(X_train, y_train)\n",
    "            val_score = self.clf.model.score(X_val, y_val)  # 分类准确率\n",
    "            best_score = val_score\n",
    "            best_model = self.clf.model\n",
    "\n",
    "            print(f\"Validation accuracy: {val_score:.4f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        self.clf.model = best_model if best_model is not None else self.clf.model\n",
    "        print(f\"Training completed. Best val acc: {best_score:.4f}\")\n",
    "\n",
    "    def save_models(self):\n",
    "        ensure_dir('models_ml')  # 改成你自己的文件夹\n",
    "        clf_path = os.path.join('models_ml', f'{self.feature}_{self.classifier_name}_clf.joblib')\n",
    "        self.clf.save(clf_path)\n",
    "        \n",
    "        # SIFT codebook 也保存到同一个目录\n",
    "        if self.feature == 'sift' and hasattr(self.fe, 'codebook') and self.fe.codebook is not None:\n",
    "            codebook_path = os.path.join('models_ml', 'sift_codebook.joblib')\n",
    "            dump(self.fe.codebook, codebook_path)\n",
    "            print('SIFT codebook saved')\n",
    "\n",
    "    def load_models(self):\n",
    "        clf_path = os.path.join('models_ml', f'{self.feature}_{self.classifier_name}_clf.joblib')\n",
    "        if os.path.exists(clf_path):\n",
    "            self.clf.load(clf_path)\n",
    "            print(f'Loaded classifier: {clf_path}')\n",
    "        else:\n",
    "            print(f'Warning: Model not found: {clf_path}')\n",
    "            \n",
    "        if self.feature == 'sift':\n",
    "            codebook_path = os.path.join('models_ml', 'sift_codebook.joblib')\n",
    "            if os.path.exists(codebook_path):\n",
    "                self.fe.codebook = load(codebook_path)\n",
    "                print('Loaded SIFT codebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfeb974",
   "metadata": {},
   "source": [
    "#### FullPipelineController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a44d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "class FullPipelineController:\n",
    "    def __init__(self, data_root='data/AgroPest12', model_dir='models_ml', results_dir='results_ml'):\n",
    "        self.data_root = data_root\n",
    "        self.model_dir = model_dir\n",
    "        self.results_dir = results_dir\n",
    "        ensure_dir(self.results_dir)\n",
    "\n",
    "        yaml_path = os.path.join(data_root, 'data.yaml')\n",
    "        with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "            self.num_classes = cfg['nc']\n",
    "            self.class_names = cfg.get('names', [f'c{i}' for i in range(self.num_classes)])\n",
    "\n",
    "        self.test_ds = YOLODataset(root_dir=data_root, split='test')\n",
    "        self.gt = self._build_gt_dict()\n",
    "\n",
    "    def _build_gt_dict(self):\n",
    "        gt = {}\n",
    "        for img_path, x1, y1, x2, y2, cls in self.test_ds.samples:\n",
    "            gt.setdefault(img_path, []).append(([x1, y1, x2, y2], cls))\n",
    "        return gt\n",
    "\n",
    "    def get_pipeline(self, feature, classifier):\n",
    "        p = MLDetectionPipeline(num_classes=self.num_classes, feature=feature, classifier=classifier)\n",
    "        p.load_models()\n",
    "        return p\n",
    "\n",
    "    def get_detector(self, detector_name, pipeline):\n",
    "        if detector_name == 'sliding':\n",
    "            return SlidingWindowDetector(pipeline.fe, pipeline.clf.wrapper.model)\n",
    "        elif detector_name == 'selective':\n",
    "            return SelectiveSearchDetector(pipeline.fe, pipeline.clf.model)\n",
    "        elif detector_name == 'mser':\n",
    "            return MSERDetector(pipeline.fe, pipeline.clf.model)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown detector: {detector_name}')\n",
    "\n",
    "    def run_detection(self, detector_name, feature, classifier):\n",
    "        pipeline = self.get_pipeline(feature, classifier)\n",
    "        detector = self.get_detector(detector_name, pipeline)\n",
    "\n",
    "        detections = {}\n",
    "        timings = {}\n",
    "        img_list = sorted(self.gt.keys())\n",
    "\n",
    "        for img_path in tqdm(img_list, desc=f'{feature}_{classifier}_{detector_name}', leave=False):\n",
    "            t0 = time.time()\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            boxes, scores, labels = detector.detect(img)\n",
    "            t1 = time.time()\n",
    "            detections[img_path] = [(list(map(int, b)), float(s), int(l)) for b, s, l in zip(boxes, scores, labels)]\n",
    "            timings[img_path] = t1 - t0\n",
    "\n",
    "        return detections, timings\n",
    "\n",
    "    def save_results(self, key, detections, timings, mAP):\n",
    "        # 只保存 mAP 和 timings（极简！）\n",
    "        result = {\n",
    "            'mAP@0.5': round(float(mAP), 4),\n",
    "            'avg_inference_time(s)': round(np.mean(list(timings.values())), 4),\n",
    "            'total_images': len(timings),\n",
    "            'total_time(s)': round(sum(timings.values()), 2),\n",
    "            'FPS': round(1 / np.mean(list(timings.values())), 2)\n",
    "        }\n",
    "        # 保存精简版\n",
    "        with open(os.path.join(self.results_dir, f'{key}_result.json'), 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        # 同时保存 detections（可选，用于后续可视化）\n",
    "        with open(os.path.join(self.results_dir, f'{key}_detections.json'), 'w') as f:\n",
    "            json.dump({'detections': detections, 'timings': timings}, f)\n",
    "\n",
    "    # ==================== 主函数：支持两种模式 ====================\n",
    "    def run(self, run_best_only=True,\n",
    "            best_feature='sift', best_classifier='rf', best_detector='selective'):\n",
    "        \"\"\"\n",
    "        run_best_only=True  → 只跑最优模型\n",
    "        run_best_only=False → 跑全部 27 个（消融实验用）\n",
    "        \"\"\"\n",
    "        if run_best_only:\n",
    "            print(\"模式：只评估最优模型\")\n",
    "            key = f'{best_feature}_{best_classifier}_{best_detector}'\n",
    "            result_file = os.path.join(self.results_dir, f'{key}_result.json')\n",
    "            if os.path.exists(result_file):\n",
    "                with open(result_file) as f:\n",
    "                    data = json.load(f)\n",
    "                print(f\"已存在 → {key} | mAP@0.5 = {data['mAP@0.5']} | FPS = {data['FPS']}\")\n",
    "                return\n",
    "\n",
    "            print(f\"正在评估最优模型：{key}\")\n",
    "            detections, timings = self.run_detection(best_detector, best_feature, best_classifier)\n",
    "            result_dict = evaluate_map(detections, self.gt, self.num_classes)\n",
    "            mAP = result_dict['mAP@0.5']\n",
    "            self.save_results(key, detections, timings, mAP)\n",
    "            print(f\"完成！最优模型 mAP@0.5 = {mAP:.4f} | FPS = {1/np.mean(list(timings.values())):.2f}\")\n",
    "\n",
    "        else:\n",
    "            print(\"模式：评估全部 27 种组合（消融实验）\")\n",
    "            features = ['hog', 'lbp', 'sift']\n",
    "            classifiers = ['svm', 'rf', 'knn']\n",
    "            detectors = ['mser', 'sliding', 'selective']\n",
    "            results = []\n",
    "\n",
    "            for det in detectors:\n",
    "                for feat in features:\n",
    "                    for clf in classifiers:\n",
    "                        key = f'{feat}_{clf}_{det}'\n",
    "                        result_file = os.path.join(self.results_dir, f'{key}_result.json')\n",
    "                        if os.path.exists(result_file):\n",
    "                            with open(result_file) as f:\n",
    "                                data = json.load(f)\n",
    "                            results.append({'model': key, 'mAP@0.5': data['mAP@0.5'], 'FPS': data['FPS']})\n",
    "                            print(f\"已完成: {key} → mAP@0.5 = {data['mAP@0.5']}\")\n",
    "                            continue\n",
    "\n",
    "                        print(f\"Running → {key}\")\n",
    "                        try:\n",
    "                            detections, timings = self.run_detection(det, feat, clf)\n",
    "                            result_dict = evaluate_map(detections, self.gt, self.num_classes)\n",
    "                            mAP = result_dict['mAP@0.5']\n",
    "                            self.save_results(key, detections, timings, mAP)\n",
    "                            fps = 1 / np.mean(list(timings.values()))\n",
    "                            results.append({'model': key, 'mAP@0.5': round(mAP, 4), 'FPS': round(fps, 2)})\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error {key}: {e}\")\n",
    "\n",
    "            # 保存汇总表格\n",
    "            df = pd.DataFrame(results).sort_values('mAP@0.5', ascending=False)\n",
    "            df.to_excel(os.path.join(self.results_dir, 'ALL_RESULTS.xlsx'), index=False)\n",
    "            df.to_csv(os.path.join(self.results_dir, 'ALL_RESULTS.csv'), index=False)\n",
    "            print(\"全部完成！查看 ALL_RESULTS.xlsx\")\n",
    "            \n",
    "    def generate_final_visualizations(self, \n",
    "                                      feature='sift', \n",
    "                                      classifier='rf', \n",
    "                                      detector_name='selective',\n",
    "                                      num_success=6, \n",
    "                                      num_failure=6):\n",
    "        \"\"\"\n",
    "        1. 混淆矩阵（保存为 png）\n",
    "        2. 6 张成功案例（TP）\n",
    "        3. 6 张失败案例（FN + FP）\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import random\n",
    "        import cv2\n",
    "        \n",
    "        key = f'{feature}_{classifier}_{detector_name}'\n",
    "        det_path = os.path.join(self.results_dir, f'{key}_detections.json')\n",
    "        if not os.path.exists(det_path):\n",
    "            print(f\"未找到检测结果：{det_path}，请先跑 run()\")\n",
    "            return\n",
    "\n",
    "        with open(det_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            detections = data['detections']\n",
    "\n",
    "        # 1. 生成混淆矩阵\n",
    "        print(\"正在生成混淆矩阵...\")\n",
    "        all_gt_labels = []\n",
    "        all_pred_labels = []\n",
    "        matched_gt = set()\n",
    "\n",
    "        for img_path in detections:\n",
    "            if img_path not in self.gt:\n",
    "                continue\n",
    "            pred_boxes = detections[img_path]\n",
    "            gt_boxes = self.gt[img_path]\n",
    "\n",
    "            # 简单匹配：每个 pred 找最近的 gt（IoU > 0.5）\n",
    "            for pred_box, score, pred_label in pred_boxes:\n",
    "                best_iou = 0\n",
    "                best_gt_label = -1\n",
    "                for gt_box, gt_label in gt_boxes:\n",
    "                    iou = self._compute_iou(pred_box, gt_box)\n",
    "                    if iou > best_iou and iou > 0.5:\n",
    "                        best_iou = iou\n",
    "                        best_gt_label = gt_label\n",
    "                if best_gt_label != -1:\n",
    "                    all_gt_labels.append(best_gt_label)\n",
    "                    all_pred_labels.append(pred_label)\n",
    "                    matched_gt.add((img_path, tuple(gt_box)))\n",
    "\n",
    "        if len(all_gt_labels) > 0:\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(all_gt_labels, all_pred_labels, labels=list(range(self.num_classes)))\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            import seaborn as sns\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "            plt.title(f'Confusion Matrix - {key}')\n",
    "            plt.ylabel('Ground Truth')\n",
    "            plt.xlabel('Prediction')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.results_dir, f'{key}_confusion_matrix.png'), dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"混淆矩阵已保存：{key}_confusion_matrix.png\")\n",
    "        else:\n",
    "            print(\"无匹配框，无法生成混淆矩阵\")\n",
    "\n",
    "        # 2. 可视化成功和失败案例\n",
    "        print(\"正在挑选成功和失败案例...\")\n",
    "        success_cases = []\n",
    "        failure_cases = []\n",
    "\n",
    "        img_paths = list(self.gt.keys())\n",
    "        random.shuffle(img_paths)\n",
    "\n",
    "        for img_path in img_paths:\n",
    "            if len(success_cases) >= num_success and len(failure_cases) >= num_failure:\n",
    "                break\n",
    "\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            h, w = img.shape[:2]\n",
    "\n",
    "            gt_boxes = [b for b, c in self.gt[img_path]]\n",
    "            gt_labels = [c for b, c in self.gt[img_path]]\n",
    "\n",
    "            pred_items = detections.get(img_path, [])\n",
    "            pred_boxes = [b for b, s, l in pred_items]\n",
    "            pred_labels = [l for b, s, l in pred_items]\n",
    "\n",
    "            # 计算 TP、FN、FP\n",
    "            tp = 0\n",
    "            matched = set()\n",
    "            for i, pred_box in enumerate(pred_boxes):\n",
    "                for j, gt_box in enumerate(gt_boxes):\n",
    "                    if self._compute_iou(pred_box, gt_box) > 0.5 and pred_labels[i] == gt_labels[j]:\n",
    "                        tp += 1\n",
    "                        matched.add(j)\n",
    "                        break\n",
    "\n",
    "            fn = len(gt_boxes) - len(matched)\n",
    "            fp = len(pred_boxes) - tp\n",
    "\n",
    "            # 画图函数\n",
    "            def draw_and_save(boxes, labels, color, title_prefix):\n",
    "                img_copy = img.copy()\n",
    "                for box, label in zip(boxes, labels):\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    cv2.rectangle(img_copy, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(img_copy, self.class_names[label], (x1, y1-10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                return img_copy\n",
    "\n",
    "            if tp >= 2 and len(success_cases) < num_success:\n",
    "                vis = draw_and_save(gt_boxes, gt_labels, (0, 255, 0), \"GT\")\n",
    "                vis = draw_and_save(pred_boxes, pred_labels, (255, 0, 0), \"Pred\")\n",
    "                cv2.putText(vis, \"SUCCESS\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "                success_cases.append((vis, img_path))\n",
    "\n",
    "            if (fn >= 1 or fp >= 2) and len(failure_cases) < num_failure:\n",
    "                vis = draw_and_save(gt_boxes, gt_labels, (0, 255, 0), \"GT\")\n",
    "                vis = draw_and_save(pred_boxes, pred_labels, (255, 0, 0), \"Pred\")\n",
    "                cv2.putText(vis, \"FAILURE\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                failure_cases.append((vis, img_path))\n",
    "\n",
    "        # 保存图片\n",
    "        vis_dir = os.path.join(self.results_dir, 'visualizations')\n",
    "        ensure_dir(vis_dir)\n",
    "        for i, (img_vis, path) in enumerate(success_cases):\n",
    "            cv2.imwrite(os.path.join(vis_dir, f'success_{i+1}_{os.path.basename(path)}'), img_vis)\n",
    "        for i, (img_vis, path) in enumerate(failure_cases):\n",
    "            cv2.imwrite(os.path.join(vis_dir, f'failure_{i+1}_{os.path.basename(path)}'), img_vis)\n",
    "\n",
    "        print(f\"可视化完成！共 {len(success_cases)} 张成功案例 + {len(failure_cases)} 张失败案例\")\n",
    "        print(f\"所有图片保存在：{vis_dir}\")\n",
    "\n",
    "    def _compute_iou(self, box1, box2):\n",
    "        x1, y1, x2, y2 = box1\n",
    "        x1g, y1g, x2g, y2g = box2\n",
    "        xi1 = max(x1, x1g)\n",
    "        yi1 = max(y1, y1g)\n",
    "        xi2 = min(x2, x2g)\n",
    "        yi2 = min(y2, y2g)\n",
    "        if xi2 <= xi1 or yi2 <= yi1:\n",
    "            return 0.0\n",
    "        inter_area = (xi2 - xi1) * (yi2 - yi1)\n",
    "        box1_area = (x2 - x1) * (y2 - y1)\n",
    "        box2_area = (x2g - x1g) * (y2g - y1g)\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        return inter_area / union_area if union_area > 0 else 0.0\n",
    "    \n",
    "    def quick_test_accuracy(self, feature='sift', classifier='rf', detector_name='selective'):\n",
    "        \"\"\"快速测试现有模型的准确率\"\"\"\n",
    "        key = f'{feature}_{classifier}_{detector_name}'\n",
    "        det_path = os.path.join(self.results_dir, f'{key}_detections.json')\n",
    "        \n",
    "        if not os.path.exists(det_path):\n",
    "            print(f\"未找到检测结果，请先运行检测：{det_path}\")\n",
    "            return\n",
    "        \n",
    "        # 加载检测结果\n",
    "        with open(det_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            detections = data['detections']\n",
    "        \n",
    "        # 计算准确率\n",
    "        accuracy_results = self.calculate_accuracy(detections)\n",
    "        \n",
    "        print(f\"=== {key} 准确率分析 ===\")\n",
    "        print(f\"总体准确率: {accuracy_results['overall_accuracy']:.4f}\")\n",
    "        print(f\"匹配检测框总数: {accuracy_results['total_matched_detections']}\")\n",
    "        print(f\"正确分类数: {accuracy_results['correct_predictions']}\")\n",
    "        print(\"\\n各类别准确率:\")\n",
    "        for class_name, acc in accuracy_results['class_accuracy'].items():\n",
    "            print(f\"  {class_name}: {acc:.4f}\")\n",
    "\n",
    "    def calculate_accuracy(self, detections, iou_threshold=0.5):\n",
    "        \"\"\"\n",
    "        计算分类准确率\n",
    "        对于每个匹配的检测框（IoU > threshold），检查分类是否正确\n",
    "        \"\"\"\n",
    "        correct_predictions = 0\n",
    "        total_matched_detections = 0\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        \n",
    "        for img_path in detections:\n",
    "            if img_path not in self.gt:\n",
    "                continue\n",
    "                \n",
    "            pred_items = detections[img_path]  # [(box, score, label), ...]\n",
    "            gt_items = self.gt[img_path]       # [(box, class), ...]\n",
    "            \n",
    "            # 创建匹配标记\n",
    "            gt_matched = [False] * len(gt_items)\n",
    "            \n",
    "            # 对每个预测框，找到最佳匹配的真实框\n",
    "            for pred_box, score, pred_label in pred_items:\n",
    "                best_iou = 0\n",
    "                best_gt_idx = -1\n",
    "                \n",
    "                for gt_idx, (gt_box, gt_label) in enumerate(gt_items):\n",
    "                    if gt_matched[gt_idx]:\n",
    "                        continue\n",
    "                        \n",
    "                    iou = self._compute_iou(pred_box, gt_box)\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = gt_idx\n",
    "                \n",
    "                # 如果找到匹配且IoU大于阈值\n",
    "                if best_gt_idx != -1 and best_iou >= iou_threshold:\n",
    "                    gt_matched[best_gt_idx] = True\n",
    "                    gt_label = gt_items[best_gt_idx][1]\n",
    "                    \n",
    "                    # 记录预测和真实标签\n",
    "                    all_predictions.append(pred_label)\n",
    "                    all_true_labels.append(gt_label)\n",
    "                    \n",
    "                    # 检查分类是否正确\n",
    "                    if pred_label == gt_label:\n",
    "                        correct_predictions += 1\n",
    "                    total_matched_detections += 1\n",
    "        \n",
    "        # 计算准确率\n",
    "        accuracy = correct_predictions / total_matched_detections if total_matched_detections > 0 else 0\n",
    "        \n",
    "        # 计算每个类别的准确率\n",
    "        class_correct = [0] * self.num_classes\n",
    "        class_total = [0] * self.num_classes\n",
    "        \n",
    "        for pred, true in zip(all_predictions, all_true_labels):\n",
    "            class_total[true] += 1\n",
    "            if pred == true:\n",
    "                class_correct[true] += 1\n",
    "        \n",
    "        class_accuracy = {}\n",
    "        for i in range(self.num_classes):\n",
    "            if class_total[i] > 0:\n",
    "                class_accuracy[self.class_names[i]] = class_correct[i] / class_total[i]\n",
    "            else:\n",
    "                class_accuracy[self.class_names[i]] = 0\n",
    "        \n",
    "        return {\n",
    "            'overall_accuracy': round(accuracy, 4),\n",
    "            'total_matched_detections': total_matched_detections,\n",
    "            'correct_predictions': correct_predictions,\n",
    "            'class_accuracy': class_accuracy,\n",
    "            'all_predictions': all_predictions,\n",
    "            'all_true_labels': all_true_labels\n",
    "        }\n",
    "\n",
    "    def save_results_with_accuracy(self, key, detections, timings, mAP, accuracy_results):\n",
    "        \"\"\"保存结果，包含准确率信息\"\"\"\n",
    "        result = {\n",
    "            'mAP@0.5': round(float(mAP), 4),\n",
    "            'accuracy@0.5': accuracy_results['overall_accuracy'],\n",
    "            'avg_inference_time(s)': round(np.mean(list(timings.values())), 4),\n",
    "            'total_images': len(timings),\n",
    "            'total_time(s)': round(sum(timings.values()), 2),\n",
    "            'FPS': round(1 / np.mean(list(timings.values())), 2),\n",
    "            'total_matched_detections': accuracy_results['total_matched_detections'],\n",
    "            'correct_predictions': accuracy_results['correct_predictions'],\n",
    "            'class_accuracy': accuracy_results['class_accuracy']\n",
    "        }\n",
    "        \n",
    "        # 保存精简版结果\n",
    "        with open(os.path.join(self.results_dir, f'{key}_result.json'), 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        \n",
    "        # 保存详细检测结果（用于后续分析）\n",
    "        with open(os.path.join(self.results_dir, f'{key}_detections.json'), 'w') as f:\n",
    "            json.dump({\n",
    "                'detections': detections, \n",
    "                'timings': timings,\n",
    "                'accuracy_details': {\n",
    "                    'all_predictions': accuracy_results['all_predictions'],\n",
    "                    'all_true_labels': accuracy_results['all_true_labels']\n",
    "                }\n",
    "            }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39169b6",
   "metadata": {},
   "source": [
    "#### mAP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a62f4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap(recall, precision):\n",
    "    # 11-point interpolation or more modern continuous AP\n",
    "    # We'll use VOC 2010+ style: integrate precision-recall curve\n",
    "    # make precision monotonically decreasing\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "    for i in range(len(mpre)-1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "    # area under curve\n",
    "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = np.sum((mrec[idx+1] - mrec[idx]) * mpre[idx+1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_map(detections, ground_truths, num_classes, iou_thresh=0.5):\n",
    "    APs = []\n",
    "    for cls in range(num_classes):\n",
    "        det_list = []\n",
    "        gt_count = 0\n",
    "        gt_by_image = {}\n",
    "        for img, gts in ground_truths.items():\n",
    "            gboxes = [b for b, c in gts if c == cls]\n",
    "            gt_count += len(gboxes)\n",
    "            gt_by_image[img] = {'boxes': gboxes, 'det': np.zeros(len(gboxes), dtype=bool)}\n",
    "        \n",
    "        for img, dets in detections.items():\n",
    "            for box, score, c in dets:\n",
    "                if c == cls:\n",
    "                    det_list.append((img, score, box))\n",
    "\n",
    "        if gt_count == 0:\n",
    "            APs.append(float('nan'))\n",
    "            continue\n",
    "\n",
    "        det_list = sorted(det_list, key=lambda x: x[1], reverse=True)\n",
    "        tp = np.zeros(len(det_list))\n",
    "        fp = np.zeros(len(det_list))\n",
    "\n",
    "        for i, (img, score, box) in enumerate(det_list):\n",
    "            ginfo = gt_by_image.get(img, {'boxes': [], 'det': np.array([])})\n",
    "            if len(ginfo['boxes']) == 0:\n",
    "                fp[i] = 1\n",
    "                continue\n",
    "            best_iou = 0.0\n",
    "            best_idx = -1\n",
    "            for j, gbox in enumerate(ginfo['boxes']):\n",
    "                _iou = iou(box, gbox)\n",
    "                if _iou > best_iou:\n",
    "                    best_iou = _iou\n",
    "                    best_idx = j\n",
    "            if best_iou >= iou_thresh and best_idx >= 0:\n",
    "                if not ginfo['det'][best_idx]:\n",
    "                    tp[i] = 1\n",
    "                    ginfo['det'][best_idx] = True\n",
    "                else:\n",
    "                    fp[i] = 1\n",
    "            else:\n",
    "                fp[i] = 1\n",
    "\n",
    "        fp_cum = np.cumsum(fp)\n",
    "        tp_cum = np.cumsum(tp)\n",
    "        recall = tp_cum / float(gt_count)\n",
    "        precision = tp_cum / np.maximum(tp_cum + fp_cum, np.finfo(np.float64).eps)\n",
    "        ap = compute_ap(recall, precision)\n",
    "        APs.append(ap)\n",
    "\n",
    "    valid_aps = [a for a in APs if not math.isnan(a)]\n",
    "    mAP = np.mean(valid_aps) if valid_aps else 0.0\n",
    "\n",
    "    return {\n",
    "        'APs': APs,\n",
    "        'mAP': mAP,\n",
    "        'mAP@0.5': mAP,\n",
    "        'mAP@0.5:0.95': mAP  # 占位\n",
    "    }\n",
    "\n",
    "def compute_detection_metrics(detections, ground_truths, num_classes, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    输入：detections 和 gt 字典\n",
    "    输出：各类指标（macro平均） + 每张图的 TP/FP/FN 统计\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_scores = []          # 用于 AUC\n",
    "    all_tp = defaultdict(int)\n",
    "    all_fp = defaultdict(int)\n",
    "    all_fn = defaultdict(int)\n",
    "\n",
    "    # 为每张图构建 gt 标记状态\n",
    "    gt_used = {}\n",
    "    for img, gts in ground_truths.items():\n",
    "        gt_used[img] = {i: False for i in range(len(gts))}\n",
    "\n",
    "    # 收集所有检测（按置信度排序）\n",
    "    all_dets = []\n",
    "    for img, dets in detections.items():\n",
    "        for box, score, cls in dets:\n",
    "            all_dets.append((img, score, box, cls))\n",
    "    all_dets.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for img, score, box, pred_cls in all_dets:\n",
    "        if img not in ground_truths:\n",
    "            y_true.append(-1)           # 背景\n",
    "            y_pred.append(pred_cls)\n",
    "            y_scores.append(score)\n",
    "            all_fp[pred_cls] += 1\n",
    "            continue\n",
    "\n",
    "        best_iou = 0\n",
    "        best_idx = -1\n",
    "        best_gt_cls = -1\n",
    "        for idx, (gt_box, gt_cls) in enumerate(ground_truths[img]):\n",
    "            if gt_used[img][idx]:\n",
    "                continue\n",
    "            _iou = iou(box, gt_box)\n",
    "            if _iou > best_iou:\n",
    "                best_iou = _iou\n",
    "                best_idx = idx\n",
    "                best_gt_cls = gt_cls\n",
    "\n",
    "        if best_iou >= iou_thresh and pred_cls == best_gt_cls:\n",
    "            # TP\n",
    "            y_true.append(pred_cls)\n",
    "            y_pred.append(pred_cls)\n",
    "            y_scores.append(score)\n",
    "            all_tp[pred_cls] += 1\n",
    "            gt_used[img][best_idx] = True\n",
    "        else:\n",
    "            # FP\n",
    "            y_true.append(-1)\n",
    "            y_pred.append(pred_cls)\n",
    "            y_scores.append(score)\n",
    "            all_fp[pred_cls] += 1\n",
    "\n",
    "    # 统计漏检 FN\n",
    "    for img, used_dict in gt_used.items():\n",
    "        for idx, used in used_dict.items():\n",
    "            if not used:\n",
    "                gt_cls = ground_truths[img][idx][1]\n",
    "                all_fn[gt_cls] += 1\n",
    "                y_true.append(gt_cls)\n",
    "                y_pred.append(-1)   # 没预测出来\n",
    "\n",
    "    # 计算指标（忽略背景类 -1）\n",
    "    valid = np.array(y_true) >= 0\n",
    "    if valid.sum() == 0:\n",
    "        return {k: 0.0 for k in ['precision','recall','f1','accuracy','auc']}, all_tp, all_fp, all_fn\n",
    "\n",
    "    yt = np.array(y_true)[valid]\n",
    "    yp = np.array(y_pred)[valid]\n",
    "    ys = np.array(y_scores)[valid]\n",
    "\n",
    "    metrics = {\n",
    "        'precision': precision_score(yt, yp, average='macro', zero_division=0),\n",
    "        'recall'   : recall_score(yt, yp, average='macro', zero_division=0),\n",
    "        'f1'       : f1_score(yt, yp, average='macro', zero_division=0),\n",
    "        'accuracy' : accuracy_score(yt, yp),\n",
    "        'auc'      : roc_auc_score(yt, ys, average='macro', multi_class='ovr')\n",
    "    }\n",
    "    return metrics, all_tp, all_fp, all_fn\n",
    "\n",
    "\n",
    "def save_confusion_matrix(detections, ground_truths, num_classes, class_names, save_path, prefix):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # 标记已匹配的 gt\n",
    "    matched_gt = {}\n",
    "    for img in ground_truths:\n",
    "        matched_gt[img] = [False] * len(ground_truths[img])\n",
    "\n",
    "    # 遍历所有预测（按分数从高到低已排序）\n",
    "    all_dets = []\n",
    "    for img, dets in detections.items():\n",
    "        for box, score, cls in dets:\n",
    "            all_dets.append((score, img, box, cls))\n",
    "    all_dets.sort(reverse=True)  # 降序\n",
    "\n",
    "    for score, img, box, pred_cls in all_dets:\n",
    "        if img not in ground_truths:\n",
    "            continue\n",
    "        best_iou = 0\n",
    "        best_j = -1\n",
    "        best_gt_cls = -1\n",
    "        for j, (gt_box, gt_cls) in enumerate(ground_truths[img]):\n",
    "            if matched_gt[img][j]:\n",
    "                continue\n",
    "            _iou = iou(box, gt_box)\n",
    "            if _iou > best_iou:\n",
    "                best_iou = _iou\n",
    "                best_j = j\n",
    "                best_gt_cls = gt_cls\n",
    "\n",
    "        if best_iou >= 0.5 and pred_cls == best_gt_cls:\n",
    "            y_true.append(best_gt_cls)\n",
    "            y_pred.append(pred_cls)\n",
    "            matched_fp = False\n",
    "            matched_gt[img][best_j] = True\n",
    "        # FP 不加入混淆矩阵（只看匹配上的）\n",
    "\n",
    "    # 添加漏检 FN\n",
    "    for img in ground_truths:\n",
    "        for j, (gt_box, gt_cls) in enumerate(ground_truths[img]):\n",
    "            if not matched_gt[img][j]:\n",
    "                y_true.append(gt_cls)\n",
    "                y_pred.append(-1)  # 漏检\n",
    "\n",
    "    # 移除漏检，只画正确匹配的部分\n",
    "    mask = np.array(y_pred) != -1\n",
    "    y_true = np.array(y_true)[mask]\n",
    "    y_pred = np.array(y_pred)[mask]\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        print(f\"{prefix} 无匹配样本，跳过混淆矩阵\")\n",
    "        return\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # 保存 CSV\n",
    "    cm_df.to_csv(f\"{save_path}_{prefix}_cm.csv\")\n",
    "\n",
    "    # 画图\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "                xticklabels=class_names, yticklabels=class_names, linewidths=.5)\n",
    "    plt.title(f'Confusion Matrix - {prefix}\\n(IoU≥0.5 matched only, {len(y_true)} samples)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_path}_{prefix}_cm.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"{prefix} 混淆矩阵已保存 ({len(y_true)} 匹配样本)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c777c7",
   "metadata": {},
   "source": [
    "####  Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "408b5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data/AgroPest12' \n",
    "ensure_dir('models_ml')\n",
    "ensure_dir('results_ml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7f0d96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练 7 个模型（使用 valid 集选最佳模型）...\n",
      "\n",
      "Training hog+svm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 15282/15282 [01:47<00:00, 141.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 15282 samples for hog+svm\n",
      "Extracting validation features for hog+svm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:09<00:00, 143.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for hog+svm\n",
      "Applying PCA for high-dim feature (1764 → ~150 dims)\n",
      "Training with validation set selection...\n",
      "Training SVM on 15282 samples, 1764 dims...Done\n",
      "Validation accuracy: 0.3087\n",
      "Training completed. Best val acc: 0.3087\n",
      "hog+svm 训练完成并保存！\n",
      "\n",
      "Training hog+rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 15282/15282 [01:35<00:00, 159.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 15282 samples for hog+rf\n",
      "Extracting validation features for hog+rf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:08<00:00, 156.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for hog+rf\n",
      "Applying PCA for high-dim feature (1764 → ~150 dims)\n",
      "Training with validation set selection...\n",
      "Training RF on 15282 samples, 1764 dims...Done\n",
      "Validation accuracy: 0.2796\n",
      "Training completed. Best val acc: 0.2796\n",
      "hog+rf 训练完成并保存！\n",
      "\n",
      "Training hog+knn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 15282/15282 [01:39<00:00, 154.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 15282 samples for hog+knn\n",
      "Extracting validation features for hog+knn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:08<00:00, 153.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for hog+knn\n",
      "Training with validation set selection...\n",
      "Training KNN on 15282 samples, 1764 dims...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29546\\anaconda3\\envs\\yolov8\\lib\\site-packages\\threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.1984\n",
      "Training completed. Best val acc: 0.1984\n",
      "hog+knn 训练完成并保存！\n",
      "\n",
      "Training lbp+svm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 15282/15282 [01:36<00:00, 158.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 15282 samples for lbp+svm\n",
      "Extracting validation features for lbp+svm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:06<00:00, 194.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for lbp+svm\n",
      "Training with validation set selection...\n",
      "Training SVM on 15282 samples, 10 dims...Done\n",
      "Validation accuracy: 0.2319\n",
      "Training completed. Best val acc: 0.2319\n",
      "lbp+svm 训练完成并保存！\n",
      "\n",
      "Training lbp+rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 15282/15282 [01:45<00:00, 144.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 15282 samples for lbp+rf\n",
      "Extracting validation features for lbp+rf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:09<00:00, 141.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for lbp+rf\n",
      "Training with validation set selection...\n",
      "Training RF on 15282 samples, 10 dims...Done\n",
      "Validation accuracy: 0.2610\n",
      "Training completed. Best val acc: 0.2610\n",
      "lbp+rf 训练完成并保存！\n",
      "\n",
      "Training lbp+knn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 15282/15282 [01:53<00:00, 134.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 15282 samples for lbp+knn\n",
      "Extracting validation features for lbp+knn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:09<00:00, 142.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for lbp+knn\n",
      "Training with validation set selection...\n",
      "Training KNN on 15282 samples, 10 dims...Done\n",
      "Validation accuracy: 0.2185\n",
      "Training completed. Best val acc: 0.2185\n",
      "lbp+knn 训练完成并保存！\n",
      "\n",
      "Training sift+rf\n",
      "Loaded shared SIFT codebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 15282/15282 [02:04<00:00, 122.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 15282 samples for sift+rf\n",
      "Extracting validation features for sift+rf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:11<00:00, 118.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for sift+rf\n",
      "Training with validation set selection...\n",
      "Training RF on 15282 samples, 256 dims...Done\n",
      "Validation accuracy: 0.3274\n",
      "Training completed. Best val acc: 0.3274\n",
      "SIFT codebook saved\n",
      "sift+rf 训练完成并保存！\n"
     ]
    }
   ],
   "source": [
    "print(\"开始训练 7 个模型（使用 valid 集选最佳模型）...\")\n",
    "\n",
    "# 所有特征+分类器组合\n",
    "for feat in ['hog', 'lbp', 'sift']:\n",
    "    for clf in ['svm', 'rf', 'knn']:\n",
    "        # SIFT 太慢，只训练最强的 rf（可自行调整）\n",
    "        if feat == 'sift' and clf != 'rf':\n",
    "            continue\n",
    "            \n",
    "        print(f'\\nTraining {feat}+{clf}')\n",
    "        pipe = MLDetectionPipeline(num_classes=12, feature=feat, classifier=clf, sift_k=256)\n",
    "        \n",
    "        # 1. 提取训练集特征\n",
    "        X_train, y_train = pipe.build_training_data_from_annotations(\n",
    "            data_root=DATA_ROOT, split='train')\n",
    "        if len(X_train) == 0:\n",
    "            print('No train samples!')\n",
    "            continue\n",
    "\n",
    "        # 2. 提取验证集特征（用于选模型）\n",
    "        print(f\"Extracting validation features for {feat}+{clf}...\")\n",
    "        X_val, y_val = pipe.build_training_data_from_annotations(\n",
    "            data_root=DATA_ROOT, split='valid')  # 关键！用 valid 集\n",
    "\n",
    "        if len(X_val) == 0:\n",
    "            print(\"No validation samples, training without validation...\")\n",
    "            pipe.train(X_train, y_train)\n",
    "        else:\n",
    "            # 3. 训练并用验证集选最佳模型\n",
    "            pipe.train(X_train, y_train, X_val=X_val, y_val=y_val)\n",
    "        \n",
    "        pipe.save_models()\n",
    "        print(f\"{feat}+{clf} 训练完成并保存！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0817285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifier: models_ml\\hog_svm_clf.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:10<00:00, 127.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for hog+svm\n",
      "Loaded classifier: models_ml\\hog_rf_clf.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:09<00:00, 137.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for hog+rf\n",
      "Loaded classifier: models_ml\\hog_knn_clf.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:10<00:00, 123.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for hog+knn\n",
      "Loaded classifier: models_ml\\lbp_svm_clf.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:09<00:00, 134.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for lbp+svm\n",
      "Loaded classifier: models_ml\\lbp_rf_clf.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:10<00:00, 123.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for lbp+rf\n",
      "Loaded classifier: models_ml\\lbp_knn_clf.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:10<00:00, 127.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for lbp+knn\n",
      "Loaded classifier: models_ml\\sift_rf_clf.joblib\n",
      "Loaded SIFT codebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1341/1341 [00:12<00:00, 109.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1341 samples for sift+rf\n",
      "    Method  Val_Acc  Val_Precision  Val_Recall  Val_F1  Val_AUC\n",
      "0  hog_svm   0.3087         0.2747      0.2787  0.2553   0.7564\n",
      "1   hog_rf   0.2796         0.2979      0.2604  0.2458   0.7371\n",
      "2  hog_knn   0.1984         0.3526      0.2163  0.1858   0.6473\n",
      "3  lbp_svm   0.2319         0.2031      0.2022  0.1619   0.6984\n",
      "4   lbp_rf   0.2610         0.2353      0.2507  0.2378   0.7421\n",
      "5  lbp_knn   0.2185         0.2070      0.2097  0.2061   0.6471\n",
      "6  sift_rf   0.3274         0.3333      0.3069  0.2833   0.7623\n",
      "\n",
      "分类性能表格已保存！\n"
     ]
    }
   ],
   "source": [
    "def report_classification_performance_on_valid():\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "    \n",
    "    results = []\n",
    "    for feat in ['hog', 'lbp', 'sift']:\n",
    "        for clf in ['svm', 'rf', 'knn']:\n",
    "            model_path = f'models_ml/{feat}_{clf}_clf.joblib'\n",
    "            if not os.path.exists(model_path):\n",
    "                continue\n",
    "                \n",
    "            pipe = MLDetectionPipeline(num_classes=12, feature=feat, classifier=clf)\n",
    "            pipe.load_models()\n",
    "            \n",
    "            X_val, y_val = pipe.build_training_data_from_annotations(\n",
    "                data_root='data/AgroPest12', split='valid')\n",
    "            if len(X_val) == 0: continue\n",
    "            \n",
    "            y_pred = pipe.clf.model.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            p, r, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='macro', zero_division=0)\n",
    "            \n",
    "            auc = 'N/A'\n",
    "            if hasattr(pipe.clf.model, 'predict_proba'):\n",
    "                prob = pipe.clf.model.predict_proba(X_val)\n",
    "                try:\n",
    "                    auc = roc_auc_score(y_val, prob, multi_class='ovr', average='macro')\n",
    "                    auc = round(auc, 4)\n",
    "                except:\n",
    "                    auc = 'N/A'\n",
    "            \n",
    "            results.append({\n",
    "                'Method': f'{feat}_{clf}',\n",
    "                'Val_Acc': round(acc, 4),\n",
    "                'Val_Precision': round(p, 4),\n",
    "                'Val_Recall': round(r, 4),\n",
    "                'Val_F1': round(f1, 4),\n",
    "                'Val_AUC': auc\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('results_ml/classification_performance_on_VALID.csv', index=False)\n",
    "    print(df)\n",
    "    print(\"\\n分类性能表格已保存！\")\n",
    "\n",
    "# 训练完所有模型后，执行：\n",
    "report_classification_performance_on_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f4d786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = FullPipelineController(data_root='data/AgroPest12')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ab87308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模式：只评估最优模型\n",
      "已存在 → sift_rf_selective | mAP@0.5 = 0.038 | FPS = 0.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "controller.run(run_best_only=True,\n",
    "                best_feature='sift',\n",
    "                best_classifier='rf',\n",
    "                best_detector='selective') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb928213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sift_rf_selective 准确率分析 ===\n",
      "总体准确率: 0.3746\n",
      "匹配检测框总数: 283\n",
      "正确分类数: 106\n",
      "\n",
      "各类别准确率:\n",
      "  Ants: 0.5600\n",
      "  Bees: 0.3077\n",
      "  Beetles: 0.0000\n",
      "  Caterpillars: 0.2083\n",
      "  Earthworms: 0.3636\n",
      "  Earwigs: 0.0000\n",
      "  Grasshoppers: 0.5000\n",
      "  Moths: 0.7297\n",
      "  Slugs: 0.0833\n",
      "  Snails: 0.4783\n",
      "  Wasps: 0.3548\n",
      "  Weevils: 0.4444\n"
     ]
    }
   ],
   "source": [
    "# 使用快速测试\n",
    "controller.quick_test_accuracy('sift', 'rf', 'selective')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
